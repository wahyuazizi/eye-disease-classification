# -*- coding: utf-8 -*-
"""Eyes_Disease_Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/wahyuazizi/eyes-disease-classification.62b30d25-0a2e-4480-81a1-82476ff72f21.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250516/auto/storage/goog4_request%26X-Goog-Date%3D20250516T120043Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D745cb7b252bd62b2a9805ea579e83a65572490f4811b3f7bb20b83c10079a054ef5031e00f64cd1462bafadb50157ad161efb1f6e0eee4be4fd2ff20a05b62b5075d1d8d531449f8167726cc341d7a191f93d06739589e43afaee7afe4ebf874e987e531b157dab29c7042827c5f4f5bf78926c5f628e8676c9e3434b8db99d01f3ffe18f2c0131d375a5b26c2e369f7f38b23cdde69bb19bc636c7b1a953c02ad284e4e60f1c1f4d7abc0e2305892e2ebf105efc40fcfcb747b8b594802b5450743bf8ee2c3722002c801a8625fe098d45d46e6ecdb4c12586e0d8838808cc3d9fa5325e5cae38720661dcc6f723790e5d8151f2305faa31fe93ac4768ba2fa

# Proyek Klasifikasi Gambar: [Input Nama Dataset]
- **Nama:** Wahyu Azizi
- **Email:** wahyuazizi03@gmail.com
- **ID Dicoding:** wahyuazizi

## Import Semua Packages/Library yang Digunakan
"""

# Library yang sering digunakan
import os, shutil
import zipfile
import random
from random import sample
import shutil
from shutil import copyfile
import pathlib
from pathlib import Path
import numpy as np
import pandas as pd
from tqdm.notebook import tqdm as tq

# Libraries untuk pemrosesan data gambar
import cv2
from PIL import Image
import skimage
from skimage import io
from skimage.transform import resize
from skimage.transform import rotate, AffineTransform, warp
from skimage import img_as_ubyte
from skimage.exposure import adjust_gamma
from skimage.util import random_noise

# Libraries untuk pembangunan model
import keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import tensorflow as tf
from tensorflow.keras import Model, layers
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.optimizers import Adam, RMSprop, SGD
from tensorflow.keras.layers import InputLayer, Conv2D, SeparableConv2D, MaxPooling2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.applications import MobileNet
from tensorflow.keras.applications.densenet import DenseNet121
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# Mencetak versi TensorFlow yang sedang digunakan
print(tf.__version__)

"""## Data Preparation

### Data Loading
"""

from huggingface_hub import login
login()

from datasets import load_dataset

# Load dataset (hanya ada split 'train')
dataset = load_dataset("rksys/EYE_DISEASE_CLASSIFICATION")

"""### Dataset Checking"""

import random
import matplotlib.pyplot as plt
from datasets import load_dataset

# Load dataset
label_names = dataset['train'].features['label'].names

# Ambil 5 contoh acak
examples = random.sample(list(dataset['train']), 5)

# Tampilkan gambar
plt.figure(figsize=(15, 5))
for i, example in enumerate(examples):
    img = example['image']
    label = label_names[example['label']]

    plt.subplot(1, 5, i+1)
    plt.imshow(img)
    plt.title(label)
    plt.axis('off')

plt.tight_layout()
plt.show()

"""### Plot Distribusi"""

import seaborn as sns

# Ambil semua label
labels = [label_names[example['label']] for example in dataset['train']]

# Buat DataFrame
df = pd.DataFrame({'label': labels})

# Plot distribusi
plt.figure(figsize=(8,6))
sns.set_style("darkgrid")
sns.countplot(data=df, x='label', order=df['label'].value_counts().index)
plt.title('Distribusi Gambar per Kelas')
plt.xlabel('Label')
plt.ylabel('Jumlah Gambar')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""### Data Preprocessing

#### Split Dataset
"""

# Bagi manual: 80% train, 20% validation
train_valid_split = dataset['train'].train_test_split(test_size=0.2, seed=42)

train_dataset = train_valid_split['train']
valid_dataset = train_valid_split['test']
test_dataset  = dataset['test']  # ini yang dari dataset asli

"""#### Image Preprocessing"""

BATCH_SIZE = 32
IMAGE_SIZE = (150, 150)
def preprocess(example):
    image = example["image"]
    image = tf.image.resize(image, (150, 150))     # Resize dengan TensorFlow
    image = image / 255.0                          # Normalisasi ke [0, 1]
    return {"image": image, "label": example["label"]}

# Terapkan preprocessing ke setiap dataset
train_dataset = train_dataset.map(preprocess)
valid_dataset = valid_dataset.map(preprocess)
test_dataset = test_dataset.map(preprocess)

# Konversi dan batching
train_dataset = train_dataset.with_format("tensorflow")
valid_dataset = valid_dataset.with_format("tensorflow")
test_dataset = test_dataset.with_format("tensorflow")

train_tf_dataset = train_dataset.to_tf_dataset(
    columns=["image"],
    label_cols="label",
    shuffle=True,
    batch_size=BATCH_SIZE,
).prefetch(tf.data.AUTOTUNE)

val_tf_dataset = valid_dataset.to_tf_dataset(
    columns=["image"],
    label_cols="label",
    shuffle=False,
    batch_size=BATCH_SIZE,
).prefetch(tf.data.AUTOTUNE)

test_tf_dataset = test_dataset.to_tf_dataset(
    columns=["image"],
    label_cols="label",
    shuffle=False,
    batch_size=1,
).prefetch(tf.data.AUTOTUNE)

"""## Modelling"""

model = keras.Sequential([
    keras.Input(shape=(150,150,3)),
    keras.layers.Conv2D(32, (3,3), padding="same", activation="relu"),
    keras.layers.BatchNormalization(),
    keras.layers.MaxPool2D((2,2)),
    keras.layers.Conv2D(32, (3,3), padding="same", activation="relu"),
    keras.layers.BatchNormalization(),
    keras.layers.MaxPool2D((2,2)),
    keras.layers.Conv2D(32, (3,3), padding="same", activation="relu"),
    keras.layers.BatchNormalization(),
    keras.layers.MaxPool2D((2,2)),

    keras.layers.Flatten(),
    keras.layers.Dense(128, activation="relu"),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(64, activation="relu"),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(4, activation="softmax"),
])

model.compile(
    optimizer=keras.optimizers.RMSprop(),
    loss="sparse_categorical_crossentropy",
    metrics=['accuracy']
)

model.summary()

# Callback
from tensorflow.keras.callbacks import Callback

class StopTrainingAtAccuracy(Callback):
    def __init__(self, threshold=0.87):
        super().__init__()
        self.threshold = threshold

    def on_epoch_end(self, epoch, logs=None):
        accuracy = logs.get("val_accuracy")  # atau "accuracy" jika kamu tidak pakai validasi
        if accuracy is not None:
            if accuracy > self.threshold:
                print(f"\nâ¹ Training dihentikan karena val_accuracy > {self.threshold*100:.2f}%")
                self.model.stop_training = True

my_callback = [
    keras.callbacks.ModelCheckpoint(
        'best_model.h5',
        monitor='val_accuracy',
        save_best_only=True,
        mode='max'
    ),
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True
    ),
    keras.callbacks.TensorBoard(
        log_dir = './logs', update_freq='batch'
    ),
    StopTrainingAtAccuracy(threshold=0.87)
]

history = model.fit(
    train_tf_dataset,
    epochs=50,
    batch_size=BATCH_SIZE,
    validation_data=val_tf_dataset,
    callbacks=my_callback,
    verbose=1
)

"""## Evaluasi dan Visualisasi"""

# Plot training history
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy over epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss over epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# Evaluate on test dataset
test_loss, test_acc = model.evaluate(test_tf_dataset)
print(f"Test accuracy: {test_acc:.2f}")
print(f"Test loss: {test_loss:.2f}")

# Get predictions
all_labels = []
all_predictions = []

for images, labels in test_tf_dataset:
    pred = model.predict(images, verbose=0)
    pred_class = np.argmax(pred, axis=1)
    all_predictions.extend(pred_class)
    all_labels.extend(labels.numpy())

# Print Confusion Matrix
cm = confusion_matrix(all_labels, all_predictions)
cm_df = pd.DataFrame(data=cm,
                     index=label_names,
                     columns=label_names)

plt.figure(figsize=(10,8))
sns.heatmap(cm_df, annot=True, fmt="d", cmap="Blues")
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.show()

# Print Classification Report
print("\nClassification Report:")
print(classification_report(
    y_true=all_labels,
    y_pred=all_predictions,
    target_names=label_names,
    digits=4
))

"""## Konversi Model"""

# Saved Model
save_path = 'saved_model/'
tf.saved_model.save(model, save_path)
print(f"Model saved to {save_path}")

# # Save as H5 format
# model.save('eye_disease_model.h5')
# print("Model saved as eye_disease_model.h5")

!pip install tensorflowjs

# Convert ke TensorFlow.js

!tensorflowjs_converter \
    --input_format=tf_saved_model \
    --output_format=tfjs_graph_model \
    saved_model \
    tfjs_model

import os

# Convert ke TFLite
converter = tf.lite.TFLiteConverter.from_saved_model('saved_model')
tflite_model = converter.convert()

# Buat folder jika belum ada
os.makedirs('tflite', exist_ok=True)

# Simpan file .tflite
with open('tflite/model.tflite', 'wb') as f:
    f.write(tflite_model)

# Simpan label.txt
labels = ['cataract', 'diabetic_retinopathy', 'glaucoma', 'normal']
with open('tflite/label.txt', 'w') as f:
    f.write('\n'.join(labels))

"""## Inference (Optional)"""

import requests

def load_and_preprocess_image(image_path_or_url, target_size=(150, 150)):
    """
    Load dan preprocess gambar untuk inference

    Args:
        image_path: Path ke file gambar
        target_size: Ukuran target untuk resize

    Returns:
        Gambar yang sudah dipreprocess dalam bentuk array numpy
    """
    # Load gambar
    if image_path_or_url.startswith('http://') or image_path_or_url.startswith('https://'):
        response = requests.get(image_path_or_url)
        img = Image.open(BytesIO(response.content))
    else:
        img = Image.open(image_path_or_url)

    img = img.resize(target_size)
    img_array = np.array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    return img_array

def predict_eye_disease(model_path, image_path_or_url):
    """
    Melakukan prediksi penyakit mata menggunakan model yang sudah dilatih

    Args:
        model_path: Path ke model tersimpan (H5 atau SavedModel)
        image_path: Path ke gambar yang akan diprediksi

    Returns:
        Kelas prediksi dan probabilitasnya
    """
    # Daftar kelas penyakit mata
    class_names = ["cataract", "diabetic_retinopathy", "glaucoma", "normal"]

    # Load model
    try:
        # Coba load sebagai SavedModel
        model = tf.keras.models.load_model(model_path)
        print(f"Model berhasil dimuat dari {model_path}")
    except:
        # Jika gagal, coba load sebagai file H5
        model = tf.keras.models.load_model(model_path)
        print(f"Model berhasil dimuat dari {model_path}")

    # Load dan preprocess gambar
    preprocessed_img = load_and_preprocess_image(image_path)

    # Lakukan prediksi
    predictions = model.predict(preprocessed_img)

    # Dapatkan indeks kelas dengan probabilitas tertinggi
    predicted_class_index = np.argmax(predictions[0])
    predicted_class = class_names[predicted_class_index]
    confidence = float(predictions[0][predicted_class_index])

    # Visualisasi hasil
    plt.figure(figsize=(8, 6))

    # Tampilkan gambar
    plt.subplot(1, 2, 1)
    if image_path_or_url.startswith('http://') or image_path_or_url.startswith('https://'):
        response = requests.get(image_path_or_url)
        img = Image.open(BytesIO(response.content))
    else:
        img = Image.open(image_path_or_url)
    plt.imshow(img)
    plt.title("Gambar Input")
    plt.axis("off")

    # Tampilkan grafik probabilitas
    plt.subplot(1, 2, 2)
    plt.bar(class_names, predictions[0])
    plt.title("Probabilitas Prediksi")
    plt.xticks(rotation=45)
    plt.tight_layout()

    return {
        "predicted_class": predicted_class,
        "confidence": confidence,
        "probabilities": {class_names[i]: float(predictions[0][i]) for i in range(len(class_names))}
    }

from io import BytesIO
# Contoh penggunaan:
"""
Gambar didapat dalam laman berikut: https://specialtyicare.com/what-is-a-glaucoma-specialist/
"""
if __name__ == "__main__":
    # Path ke model dan gambar
    model_path = "best_model.h5"
    image_path = "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTUwLhu48VSA1vdxflYSm9QgL65ZMPu9KJMSw&s"

    # Jalankan prediksi
    result = predict_eye_disease(model_path, image_path)

    # Tampilkan hasil
    print(f"\nHasil Prediksi:")
    print(f"Kelas Prediksi: {result['predicted_class']}")
    print(f"Tingkat Keyakinan: {result['confidence']*100:.2f}%")
    print("\nProbabilitas per Kelas:")
    for cls, prob in result["probabilities"].items():
        print(f"- {cls}: {prob*100:.2f}%")

    plt.show()

"""## Prepare Requirements dan Folder"""

# Buat file requirements.txt dari pip freeze
!pip freeze > requirements.txt

!zip -r working.zip ./*

from IPython.display import FileLink
FileLink('working.zip')   # atau 'working.zip'

